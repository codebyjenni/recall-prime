{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "raw_df = pd.read_excel('06 Data analysis/00 data/dataset1_8400 tagged.xlsx',engine='openpyxl',sheet_name=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a unique identifer for each line \n",
    "import re\n",
    "raw_df.reset_index(inplace=True,drop=True)\n",
    "raw_df[\"text_id\"]=raw_df.apply(lambda row: f\"Text_{row.name+1}_{ re.sub(' ','',row['Text'][:5] )}...\"  , axis=1   )\n",
    "raw_df = raw_df[['text_id'] + [col for col in raw_df if col != 'text_id']]\n",
    "raw_df.to_csv(\"06 Data analysis/00 data/python_datasets/dataset1_text_ID_created.csv\",index=False)\n",
    "#create a text df only with necessary lines for clustering \n",
    "text_df=raw_df[[\"text_id\",\"Text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve embedding using OpenAI's text-embedding-3-large\n",
    " The embedding model we use, for example, OpanAI's embedding model here, can actually be used within the bertopic, however, we still choose to get the embedding for our dataset first and separately, and store the embeddings separately, and then pass it into the bertopic pipeline, this makes the workflow safer and more replicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the tokenizer number for each text since embedding models often have a maxium token number limit (though probably not a problem here)\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name=\"cl100k_base\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "text_df[\"token_num\"]=text_df[\"Text\"].apply(num_tokens_from_string )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine batch size\n",
    "\n",
    "def get_max_batch_size(token_num_list, TPM):\n",
    "    n = len(token_num_list)\n",
    "    for size in range(1, n+1):  \n",
    "        valid = True\n",
    "        for i in range(0, n, size):  \n",
    "            if sum(token_num_list[i:i+size]) > TPM:\n",
    "                valid = False\n",
    "                break\n",
    "        if not valid:\n",
    "            return size - 2 \n",
    "    return n  \n",
    "\n",
    "batch_size=get_max_batch_size(text_df[\"token_num\"].to_list(), 1000000) \n",
    "print(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "import numpy as np\n",
    "\n",
    "def log_before_retry(retry_state):\n",
    "    exception_message = retry_state.outcome.exception() if retry_state.outcome and retry_state.outcome.exception() else 'No exception'\n",
    "    print(f\"Preparing for Retry {retry_state.attempt_number} due to {exception_message}\")\n",
    "\n",
    "\n",
    "class OpenAI_Embeddinger:\n",
    "    def __init__(self,answer_text_ds:pd.DataFrame):\n",
    "        self.client =OpenAI() # Initialize client and model\n",
    "        self.answer_text_ds=answer_text_ds\n",
    "        self.answer_text_ds.reset_index(inplace=True,drop=True)\n",
    "        self.embedding_dict=dict()\n",
    "       \n",
    "    @retry(wait=wait_random_exponential(min=1, max=60),\n",
    "           stop=stop_after_attempt(6),\n",
    "           before_sleep=log_before_retry)\n",
    "    def get_embedding_batch(self, texts,embedding_model):\n",
    "        responses =  self.client.embeddings.create(input=texts, model=embedding_model)\n",
    "        embeddings = [None] * len(texts)  # Initialize a list to store embeddings in order\n",
    "        for response in responses.data:\n",
    "                embeddings[response.index] = np.array(response.embedding)  # Store the embedding using the index to maintain order\n",
    "        return embeddings\n",
    "\n",
    "    def append_embedding(self, answer_name_column_name:str,text_column_name: str, model:str,batch_size):\n",
    "        print(batch_size)\n",
    "        # Generate batches from the dataset\n",
    "        texts = self.answer_text_ds[text_column_name].to_list()\n",
    "        batches = [texts[i:i + batch_size] for i in range(0, len(texts), batch_size)]\n",
    "        total_batch_num = len(batches)\n",
    "        print(\"calling api started!\")\n",
    "        # Loop through each batch to process and store embeddings\n",
    "        for batch_number, batch in enumerate(batches, start=1):\n",
    "            start_index = (batch_number - 1) * batch_size\n",
    "            end_index = start_index + len(batch)-1\n",
    "            answer_names=self.answer_text_ds.loc[start_index:end_index,answer_name_column_name]\n",
    "            embeddings = self.get_embedding_batch(texts=batch, embedding_model=model)\n",
    "            # we need to store the embeddings back into the dataset\n",
    "            batch_dict= {name: array for name, array in zip(answer_names, embeddings)}\n",
    "            self.embedding_dict.update(batch_dict)\n",
    "            print(f\"Batch {batch_number}/{total_batch_num} done!\")\n",
    "\n",
    "        # Indicate the end of the function with a suitable return or simply end the function\n",
    "        print(\"All batches processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenAI_embedding=OpenAI_Embeddinger(answer_text_ds=text_df)\n",
    "OpenAI_embedding.append_embedding(\"text_id\",\"Text\",\"text-embedding-3-large\",batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"06 Data analysis/04 Topic Modeling/outputs/embeddings/ds1_text_embedding\", 'wb') as file:\n",
    "    pickle.dump(OpenAI_embedding.embedding_dict, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also save in a table together with text \n",
    "text_df[\"embedding\"]=text_df[\"text_id\"].map(OpenAI_embedding.embedding_dict)\n",
    "\n",
    "#transfer the embeddings in numpy array to list so that they can be fully saved in an csv table \n",
    "text_df[\"embedding\"]=text_df[\"embedding\"].apply(list)\n",
    "text_df.to_csv(\"06 Data analysis/04 Topic Modeling/outputs/embeddings/ds1_text_embedding_with_text.csv\",index=False )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
